export const workData = {
  electra: {
    id: 'electra',
    image: '/electra_2.png',
    wide_img: '/electra_2.png',
    company: 'Electra - A clean iron company.',
    role: 'Software Engineer - Full Stack',
    period: '5/2024 - Present',
    location: 'Boulder, CO, hybrid',
    companyUrl: 'https://electra.earth',
    compDescription: "Electra is a pioneering clean iron company focused on revolutionizing the steel industry with sustainable practices and innovative technology.",
    description: 'As a Full Stack Software Engineer at Electra, I lead the development of internal data tooling platforms that support our scientists and engineers with comprehensive data management, visualization, and reporting capabilities.',
    technologies: ['TypeScript', 'React', 'Next.js', 'FastAPI', 'PostgreSQL', 'Python', 'SQLAlchemy', 'Docker', 'OAuth 2.0', 'Azure AD', 'Airflow'],
    situation: 'Electra needed a comprehensive internal data platform to manage scientific data from various devices, improve data visibility across teams, and streamline data consumption between company resources.',
    task: 'Plan, design, and build an end-to-end internal data tool with frontend and backend components, implement secure user authentication, develop REST APIs, and create data extraction scripts for scientific devices.',
    action: 'Built a full-stack application with TypeScript and React frontend and FastAPI backend, implemented OAuth 2.0 and Azure AD for user group authorization, developed REST APIs with PostgreSQL and SQLAlchemy, wrote Python scripts for scientific device data extraction, implemented pytest unit tests, and managed workflows with Airflow.',
    result: 'Increased data visibility by 90%, enhanced data consumption between company resources by 60%, and created a scalable platform that serves as the central hub for scientific data management and analysis.',
    responsibilities: [
      'Full-stack development with TypeScript and React',
      'REST API development with FastAPI and PostgreSQL',
      'Database architecture and optimization with SQLAlchemy',
      'User authentication with OAuth 2.0 and Azure AD',
      'Python scripting for scientific device integration',
      'Task management and workflow automation with Airflow',
      'Unit testing with pytest'
    ]
  },
  wikirate: {
    id: 'wikirate',
    image: '/wikirate_2.png',
    wide_img: '/wikirate_wide.png',
    company: 'Wikirate - An open source ESG data reporting platform.',
    role: 'Software Engineer - Full Stack',
    period: '8/2023 - 3/2024',
    location: 'Berlin, Germany, remote',
    companyUrl: 'https://wikirate.org',
    compDescription: "Wikirate is an open-source platform that collects and analyzes corporate sustainability data to empower researchers, activists, and consumers with transparent information about environmental, social, and governance (ESG) practices.",
    description: 'Developed comprehensive analytics and visualization tools for corporate sustainability data while contributing to the open-source Ruby on Rails platform and improving data accuracy through quality assurance systems.',
    technologies: ['FastAPI', 'Dash', 'TypeScript', 'React', 'Ruby on Rails', 'Python', 'Pandas', 'Matplotlib'],
    situation: 'Wikirate needed enhanced analytics capabilities, improved data accuracy verification, and better UI/UX for their open-source ESG data platform to serve researchers and activists more effectively.',
    task: 'Design and build analytics and visualization engines, implement quality assurance tools, enhance the existing Ruby on Rails application, and develop web scraping capabilities for data collection.',
    action: 'Built an analytics and visualization engine using FastAPI, Dash, TypeScript, and React; implemented data analysis and visualization for Quality Assurance using Pandas and Matplotlib; contributed new features to the open-source Ruby on Rails application per grant requirements; improved UI/UX design; and developed web scrapers for official company webpage data collection.',
    result: 'Significantly improved data accuracy and accessibility for researchers, enhanced the platform\'s analytical capabilities, and contributed valuable open-source features that support evidence-based environmental advocacy.',
    responsibilities: [
      'Analytics engine development with FastAPI and Dash',
      'Data visualization with React and TypeScript',
      'Quality assurance tools with Pandas and Matplotlib',
      'Open-source Ruby on Rails development',
      'UI/UX design improvements',
      'Web scraping for data collection'
    ]
  },
  tpg: {
    id: 'tpg',
    image: '/tpg_2.png',
    wide_img: '/tpg_wide.png',
    company: 'Travelpass - Your custom travel experience.',
    role: 'Data Engineer',
    period: '1/2022 - 7/2022',
    location: 'South Jordan, UT, remote',
    companyUrl: 'https://travelpass.com',
    compDescription: "Travelpassgroup is a leading travel technology company that provides innovative solutions for personalized travel experiences, including booking platforms and analytics services for the travel industry.",
    description: 'Developed efficient ETL pipelines and data analysis systems to process large-scale travel data, enabling faster insights and decision-making while optimizing costs through cloud infrastructure improvements.',
    technologies: ['Python', 'Flask', 'PostgreSQL', 'SQL', 'AWS', 'AWS Lambda'],
    situation: 'Travelpassgroup required scalable ETL pipelines to process millions of travel data points, improve operational efficiency, and reduce infrastructure costs while maintaining data accuracy.',
    task: 'Develop efficient ETL pipelines, automate manual processes, optimize cloud infrastructure costs, and conduct comprehensive data analysis to provide actionable insights to leadership.',
    action: 'Built ETL pipelines with Python and Flask for improved data accuracy and faster insights; automated tasks through cross-functional collaboration; refactored existing jobs into AWS Lambdas for cost optimization; and conducted data analysis on 10 million data points to communicate insights to leadership.',
    result: 'Achieved a 25% increase in operational efficiency through automation, reduced infrastructure costs by 30% through AWS Lambda optimization, and provided leadership with actionable insights from large-scale data analysis.',
    responsibilities: [
      'ETL pipeline development with Python and Flask',
      'Data analysis on 10+ million data points',
      'AWS Lambda optimization for cost reduction',
      'Cross-functional automation initiatives',
      'Leadership reporting and insights communication'
    ]
  }
}